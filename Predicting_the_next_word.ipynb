{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFWbEb6uGbN-"
      },
      "source": [
        "Predicting the next word\n",
        "\n",
        "1.   Élément de liste\n",
        "2.   Élément de liste\n",
        "\n",
        "\n",
        "\n",
        "Welcome to this assignment! During this week you saw how to create a model that will predict the next word in a text sequence, now you will implement such model and train it using a corpus of Shakespeare's sonnets, while also creating some helper functions to pre-process the data.\n",
        "\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional,Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTxqlHqKHzhr"
      },
      "source": [
        "For this assignment you will be using the [Shakespeare Sonnets Dataset](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154), which contains more than 2000 lines of text extracted from Shakespeare's sonnets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ4qOUzujMP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d48d2d9-d9ee-4aca-9fc9-9eef791fa1b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=108jAePKK4R3BVYBbYJZ32JWUwxeMg20K\n",
            "To: /content/sonnets.txt\n",
            "100% 93.6k/93.6k [00:00<00:00, 95.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# sonnets.txt\n",
        "!gdown --id 108jAePKK4R3BVYBbYJZ32JWUwxeMg20K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfd-nYKij5yY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5556a6-eae0-4434-bfa1-02e5b643463d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2159 lines of sonnets\n",
            "\n",
            "The first 5 lines look like this:\n",
            "\n",
            "from fairest creatures we desire increase,\n",
            "that thereby beauty's rose might never die,\n",
            "but as the riper should by time decease,\n",
            "his tender heir might bear his memory:\n",
            "but thou, contracted to thine own bright eyes,\n"
          ]
        }
      ],
      "source": [
        "# Define path for file with sonnets\n",
        "SONNETS_FILE = './sonnets.txt'\n",
        "\n",
        "# Read the data\n",
        "with open('./sonnets.txt') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Convert to lower case and save as a list\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
        "print(f\"The first 5 lines look like this:\\n\")\n",
        "for i in range(5):\n",
        "  print(corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imB15zrSNhA1"
      },
      "source": [
        "## Tokenizing the text\n",
        "\n",
        "Now fit the Tokenizer to the corpus and save the total number of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAhM_qAZk0o5"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77-0sA46OETa"
      },
      "source": [
        "When converting the text into sequences you can use the `texts_to_sequences` method as you have done throughout this course.\n",
        "\n",
        "In the next graded function you will need to process this corpus one line at a time. Given this, it is important to keep in mind that the way you are feeding the data unto this method affects the result. Check the following example to make this clearer.\n",
        "\n",
        "The first example of the corpus is a string and looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqhPxdeXlfjh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d4c9026c-9559-46c2-dbca-fb83b5c28be7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from fairest creatures we desire increase,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "corpus[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFMP4z11O3os"
      },
      "source": [
        "If you pass this text directly into the `texts_to_sequences` method you will get an unexpected result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMSEhmbzNZCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a087a6fa-9ae9-47b2-b883-7f40d1d35a3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[],\n",
              " [],\n",
              " [58],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [6],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [17],\n",
              " [],\n",
              " [],\n",
              " []]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences(corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPZmZtpEPEeI"
      },
      "source": [
        "This happened because `texts_to_sequences` expects a list and you are providing a string. However a string is still and `iterable` in Python so you will get the word index of every character in the string.\n",
        "\n",
        "Instead you need to place the example whithin a list before passing it to the method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmgo-vXhk4nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c7248c-b6eb-4133-fb57-09a193a85d0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DU7wK-eQ5dc"
      },
      "source": [
        "Notice that you received the sequence wrapped inside a list so in order to get only the desired sequence you need to explicitly get the first item in the list like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpTy8WmIQ57P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ff3c81-c2e6-40c4-b49c-8745709fc0c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[34, 417, 877, 166, 213, 517]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tokenizer.texts_to_sequences([corpus[0]])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oqy9KjXRJ9A"
      },
      "source": [
        "## Generating n_grams\n",
        "\n",
        "Now complete the `n_gram_seqs` function below. This function receives the fitted tokenizer and the corpus (which is a list of strings) and should return a list containing the `n_gram` sequences for each line in the corpus:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pad_sequences \n",
        "This function transforms a list (of length num_samples) of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps). num_timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list.\n",
        "\n",
        "#.\n",
        "equences that are shorter than num_timesteps are padded with value until they are num_timesteps long.\n",
        "\n",
        "Sequences longer than num_timesteps are truncated so that they fit the desired length.\n",
        "\n",
        "The position where padding or truncation happens is determined by the arguments padding and truncating, respectively. Pre-padding or removing values from the beginning of the sequence is the default."
      ],
      "metadata": {
        "id": "q1_N4BmrA4ao"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy4baJMDl6kj"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: n_gram_seqs\n",
        "def n_gram_seqs(corpus, tokenizer):\n",
        " input_sequences = []\n",
        " for line in corpus:\n",
        "\t  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\t  for i in range(1, len(token_list)):\n",
        "\t\t   n_gram_sequence = token_list[:i+1]\n",
        "\t\t   input_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        " max_sequence_len = max([len(x) for x in input_sequences])\n",
        " input_sequences.append(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "\t### END CODE HERE\n",
        "\t \n",
        " return input_sequences[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlKqW2pfM7G3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4a0ffc-8326-42b9-c8f6-b6fc28d5a554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for first example look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[34, 417],\n",
              " [34, 417, 877],\n",
              " [34, 417, 877, 166],\n",
              " [34, 417, 877, 166, 213],\n",
              " [34, 417, 877, 166, 213, 517]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Test your function with one example\n",
        "first_example_sequence = n_gram_seqs([corpus[0]], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for first example look like this:\\n\")\n",
        "first_example_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HL8Ug6UU0Jt"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "n_gram sequences for first example look like this:\n",
        "\n",
        "[[34, 417],\n",
        " [34, 417, 877],\n",
        " [34, 417, 877, 166],\n",
        " [34, 417, 877, 166, 213],\n",
        " [34, 417, 877, 166, 213, 517]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[1:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVxBTfqS2jWv",
        "outputId": "0e9c2bf0-fb36-420f-a2ae-264379633807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"that thereby beauty's rose might never die,\",\n",
              " 'but as the riper should by time decease,',\n",
              " 'his tender heir might bear his memory:']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J6Gde1eC2kCK",
        "outputId": "f8d28d83-6a58-405c-ea87-6ed61e5b484c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from fairest creatures we desire increase,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtPpCcBjNc4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaddaf6c-32da-4265-83b6-e70d1d5134af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_gram sequences for next 3 examples look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 878],\n",
              " [8, 878, 134],\n",
              " [8, 878, 134, 351],\n",
              " [8, 878, 134, 351, 102],\n",
              " [8, 878, 134, 351, 102, 156],\n",
              " [8, 878, 134, 351, 102, 156, 199],\n",
              " [16, 22],\n",
              " [16, 22, 2],\n",
              " [16, 22, 2, 879],\n",
              " [16, 22, 2, 879, 61],\n",
              " [16, 22, 2, 879, 61, 30],\n",
              " [16, 22, 2, 879, 61, 30, 48],\n",
              " [16, 22, 2, 879, 61, 30, 48, 634],\n",
              " [25, 311],\n",
              " [25, 311, 635],\n",
              " [25, 311, 635, 102],\n",
              " [25, 311, 635, 102, 200],\n",
              " [25, 311, 635, 102, 200, 25],\n",
              " [25, 311, 635, 102, 200, 25, 278]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Test your function with a bigger corpus\n",
        "next_3_examples_sequence = n_gram_seqs(corpus[1:4], tokenizer)\n",
        "\n",
        "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
        "next_3_examples_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIzecMczU9UB"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "n_gram sequences for next 3 examples look like this:\n",
        "\n",
        "[[8, 878],\n",
        " [8, 878, 134],\n",
        " [8, 878, 134, 351],\n",
        " [8, 878, 134, 351, 102],\n",
        " [8, 878, 134, 351, 102, 156],\n",
        " [8, 878, 134, 351, 102, 156, 199],\n",
        " [16, 22],\n",
        " [16, 22, 2],\n",
        " [16, 22, 2, 879],\n",
        " [16, 22, 2, 879, 61],\n",
        " [16, 22, 2, 879, 61, 30],\n",
        " [16, 22, 2, 879, 61, 30, 48],\n",
        " [16, 22, 2, 879, 61, 30, 48, 634],\n",
        " [25, 311],\n",
        " [25, 311, 635],\n",
        " [25, 311, 635, 102],\n",
        " [25, 311, 635, 102, 200],\n",
        " [25, 311, 635, 102, 200, 25],\n",
        " [25, 311, 635, 102, 200, 25, 278]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx3V_RjFWQSu"
      },
      "source": [
        "Apply the `n_gram_seqs` transformation to the whole corpus and save the maximum sequence length to use it later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laMwiRUpmuSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3dee418-7056-4ae8-9ae0-5f59cb57e1eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_grams of input_sequences have length: 15462\n",
            "maximum length of sequences is: 11\n"
          ]
        }
      ],
      "source": [
        "# Apply the n_gram_seqs transformation to the whole corpus\n",
        "input_sequences = n_gram_seqs(corpus, tokenizer)\n",
        "\n",
        "# Save max length \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "\n",
        "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
        "print(f\"maximum length of sequences is: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OciMdmEdE9L"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "n_grams of input_sequences have length: 15462\n",
        "maximum length of sequences is: 11\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHY7HroqWq12"
      },
      "source": [
        "## Add padding to the sequences\n",
        "\n",
        "Now code the `pad_seqs` function which will pad any given sequences to the desired maximum length. Notice that this function receives a list of sequences and should return a numpy array with the padded sequences: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "WW1-qAZaWOhC"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: pad_seqs\n",
        "def pad_seqs(input_sequences, maxlen):\n",
        "    ### START CODE HERE\n",
        "    padded_sequences = pad_sequences(input_sequences, maxlen=maxlen, padding='pre')\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(padded_sequences)\n",
        "\n",
        "# create predictors and label\n",
        "\n",
        "\n",
        "    \n",
        "    return padded_sequences\n",
        "    ### END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqVQ0pb3YHLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58125bb0-f2fd-414b-f770-410ad3064d23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,  34, 417],\n",
              "       [  0,   0,  34, 417, 877],\n",
              "       [  0,  34, 417, 877, 166],\n",
              "       [ 34, 417, 877, 166, 213],\n",
              "       [417, 877, 166, 213, 517]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Test your function with the n_grams_seq of the first example\n",
        "first_padded_seq = pad_seqs(first_example_sequence, len(first_example_sequence))\n",
        "first_padded_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re_avDznXRnU"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "array([[  0,   0,   0,  34, 417],\n",
        "       [  0,   0,  34, 417, 877],\n",
        "       [  0,  34, 417, 877, 166],\n",
        "       [ 34, 417, 877, 166, 213],\n",
        "       [417, 877, 166, 213, 517]], dtype=int32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j56_UCOBYzZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae724d1a-cf80-4b74-81bc-bcd1ec4fdbc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
              "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
              "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
              "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
              "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
              "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
              "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
              "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
              "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
              "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
              "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
              "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
              "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
              "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
              "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
              "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
              "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
              "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
              "       [  0,  25, 311, 635, 102, 200,  25, 278]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Test your function with the n_grams_seq of the next 3 examples\n",
        "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
        "next_3_padded_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rmcDluOXcIU"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "array([[  0,   0,   0,   0,   0,   0,   8, 878],\n",
        "       [  0,   0,   0,   0,   0,   8, 878, 134],\n",
        "       [  0,   0,   0,   0,   8, 878, 134, 351],\n",
        "       [  0,   0,   0,   8, 878, 134, 351, 102],\n",
        "       [  0,   0,   8, 878, 134, 351, 102, 156],\n",
        "       [  0,   8, 878, 134, 351, 102, 156, 199],\n",
        "       [  0,   0,   0,   0,   0,   0,  16,  22],\n",
        "       [  0,   0,   0,   0,   0,  16,  22,   2],\n",
        "       [  0,   0,   0,   0,  16,  22,   2, 879],\n",
        "       [  0,   0,   0,  16,  22,   2, 879,  61],\n",
        "       [  0,   0,  16,  22,   2, 879,  61,  30],\n",
        "       [  0,  16,  22,   2, 879,  61,  30,  48],\n",
        "       [ 16,  22,   2, 879,  61,  30,  48, 634],\n",
        "       [  0,   0,   0,   0,   0,   0,  25, 311],\n",
        "       [  0,   0,   0,   0,   0,  25, 311, 635],\n",
        "       [  0,   0,   0,   0,  25, 311, 635, 102],\n",
        "       [  0,   0,   0,  25, 311, 635, 102, 200],\n",
        "       [  0,   0,  25, 311, 635, 102, 200,  25],\n",
        "       [  0,  25, 311, 635, 102, 200,  25, 278]], dtype=int32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgK-Q_micEYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b298b1-16e5-40f7-c8f9-f336edddf201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "padded corpus has shape: (15462, 11)\n"
          ]
        }
      ],
      "source": [
        "# Pad the whole corpus\n",
        "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
        "\n",
        "print(f\"padded corpus has shape: {input_sequences.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59RD1YYNc7CW"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "padded corpus has shape: (15462, 11)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbOidyPrXxf7"
      },
      "source": [
        "## Split the data into features and labels\n",
        "\n",
        "Before feeding the data into the neural network you should split it into features and labels. In this case the features will be the padded n_gram sequences with the last word removed from them and the labels will be the removed word.\n",
        "\n",
        "Complete the `features_and_labels` function below. This function expects the padded n_gram sequences as input and should return a tuple containing the features and the one hot encoded labels.\n",
        "\n",
        "Notice that the function also receives the total of words in the corpus, this parameter will be very important when one hot enconding the labels since every word in the corpus will be a label at least once. If you need a refresh of how the `to_categorical` function works take a look at the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "9WGGbYdnZdmJ"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: features_and_labels\n",
        "def features_and_labels(input_sequences, total_words):\n",
        "    ### START CODE HERE\n",
        "    features = input_sequences[:,:-1]\n",
        "    labels = input_sequences[:,-1]\n",
        "    one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return features, one_hot_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23DolaBRaIAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec64eb0f-3cae-4a00-cc4b-395d0f3a0b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels have shape: (5, 3211)\n",
            "\n",
            "features look like this:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,  34],\n",
              "       [  0,   0,  34, 417],\n",
              "       [  0,  34, 417, 877],\n",
              "       [ 34, 417, 877, 166],\n",
              "       [417, 877, 166, 213]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Test your function with the padded n_grams_seq of the first example\n",
        "first_features, first_labels = features_and_labels(first_padded_seq, total_words)\n",
        "\n",
        "print(f\"labels have shape: {first_labels.shape}\")\n",
        "print(\"\\nfeatures look like this:\\n\")\n",
        "first_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "iDy5PuuNCWq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t4yAx2UaQ43"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "labels have shape: (5, 3211)\n",
        "\n",
        "features look like this:\n",
        "\n",
        "array([[  0,   0,   0,  34],\n",
        "       [  0,   0,  34, 417],\n",
        "       [  0,  34, 417, 877],\n",
        "       [ 34, 417, 877, 166],\n",
        "       [417, 877, 166, 213]], dtype=int32)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRTuLEt3bRKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf8260-28a8-47c8-b75e-977abea2ea49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features have shape: (15462, 10)\n",
            "labels have shape: (15462, 3211)\n"
          ]
        }
      ],
      "source": [
        "# Split the whole corpus\n",
        "features, labels = features_and_labels(input_sequences, total_words)\n",
        "\n",
        "print(f\"features have shape: {features.shape}\")\n",
        "print(f\"labels have shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXSMK_HpdLns"
      },
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "features have shape: (15462, 10)\n",
        "labels have shape: (15462, 3211)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltxaOCE_aU6J"
      },
      "source": [
        "## Create the model\n",
        "\n",
        "Now you should define a model architecture capable of achieving an accuracy of at least 80%.\n",
        "\n",
        "Some hints to help you in this task:\n",
        "\n",
        "- An appropriate `output_dim` for the first layer (Embedding) is 100, this is already provided for you.\n",
        "- A Bidirectional LSTM is helpful for this particular problem.\n",
        "- The last layer should have the same number of units as the total number of words in the corpus and a softmax activation function.\n",
        "- This problem can be solved with only two layers (excluding the Embedding) so try out small architectures first."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras import regularizers"
      ],
      "metadata": {
        "id": "JJ_40UqhG8Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "XrE6kpJFfvRY"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_model\n",
        "def create_model(total_words, max_sequence_len):\n",
        "    \n",
        "    model = Sequential()\n",
        "    ### START CODE HERE\n",
        "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "    model.add(Bidirectional(LSTM(max_sequence_len, return_sequences = True)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    ### END CODE HERE\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IpX_Gu_gISk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34afc4f5-a765-4e92-814a-64d7cdbcfb1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/660\n",
            "484/484 [==============================] - 13s 9ms/step - loss: 6.9492 - accuracy: 0.0215\n",
            "Epoch 2/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 6.5262 - accuracy: 0.0218\n",
            "Epoch 3/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.4415 - accuracy: 0.0221\n",
            "Epoch 4/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.3529 - accuracy: 0.0262\n",
            "Epoch 5/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.2500 - accuracy: 0.0310\n",
            "Epoch 6/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.1695 - accuracy: 0.0321\n",
            "Epoch 7/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.1007 - accuracy: 0.0393\n",
            "Epoch 8/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 6.0352 - accuracy: 0.0388\n",
            "Epoch 9/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.9733 - accuracy: 0.0408\n",
            "Epoch 10/660\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 5.9081 - accuracy: 0.0431\n",
            "Epoch 11/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.8371 - accuracy: 0.0464\n",
            "Epoch 12/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.7518 - accuracy: 0.0487\n",
            "Epoch 13/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.6743 - accuracy: 0.0522\n",
            "Epoch 14/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.5903 - accuracy: 0.0605\n",
            "Epoch 15/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 5.5112 - accuracy: 0.0596\n",
            "Epoch 16/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.4312 - accuracy: 0.0673\n",
            "Epoch 17/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.3456 - accuracy: 0.0727\n",
            "Epoch 18/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.2686 - accuracy: 0.0794\n",
            "Epoch 19/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.1839 - accuracy: 0.0828\n",
            "Epoch 20/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.0987 - accuracy: 0.0889\n",
            "Epoch 21/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 5.0221 - accuracy: 0.0934\n",
            "Epoch 22/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.9409 - accuracy: 0.0988\n",
            "Epoch 23/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.8568 - accuracy: 0.1041\n",
            "Epoch 24/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.7755 - accuracy: 0.1072\n",
            "Epoch 25/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.6924 - accuracy: 0.1177\n",
            "Epoch 26/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.6196 - accuracy: 0.1204\n",
            "Epoch 27/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.5322 - accuracy: 0.1261\n",
            "Epoch 28/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 4.4661 - accuracy: 0.1334\n",
            "Epoch 29/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.3800 - accuracy: 0.1398\n",
            "Epoch 30/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.3177 - accuracy: 0.1495\n",
            "Epoch 31/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.2422 - accuracy: 0.1552\n",
            "Epoch 32/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.1794 - accuracy: 0.1660\n",
            "Epoch 33/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.1106 - accuracy: 0.1734\n",
            "Epoch 34/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 4.0542 - accuracy: 0.1857\n",
            "Epoch 35/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.9894 - accuracy: 0.1914\n",
            "Epoch 36/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.9291 - accuracy: 0.2030\n",
            "Epoch 37/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.8720 - accuracy: 0.2118\n",
            "Epoch 38/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.8128 - accuracy: 0.2257\n",
            "Epoch 39/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.7668 - accuracy: 0.2269\n",
            "Epoch 40/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.7226 - accuracy: 0.2397\n",
            "Epoch 41/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.6639 - accuracy: 0.2465\n",
            "Epoch 42/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 3.6181 - accuracy: 0.2553\n",
            "Epoch 43/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.5621 - accuracy: 0.2689\n",
            "Epoch 44/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.5256 - accuracy: 0.2700\n",
            "Epoch 45/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.4753 - accuracy: 0.2783\n",
            "Epoch 46/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.4397 - accuracy: 0.2890\n",
            "Epoch 47/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.3900 - accuracy: 0.2981\n",
            "Epoch 48/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.3480 - accuracy: 0.3036\n",
            "Epoch 49/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.3129 - accuracy: 0.3147\n",
            "Epoch 50/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.2694 - accuracy: 0.3163\n",
            "Epoch 51/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.2362 - accuracy: 0.3234\n",
            "Epoch 52/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.1925 - accuracy: 0.3374\n",
            "Epoch 53/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.1662 - accuracy: 0.3414\n",
            "Epoch 54/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.1139 - accuracy: 0.3489\n",
            "Epoch 55/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.0917 - accuracy: 0.3518\n",
            "Epoch 56/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 3.0497 - accuracy: 0.3569\n",
            "Epoch 57/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 3.0216 - accuracy: 0.3654\n",
            "Epoch 58/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.9957 - accuracy: 0.3754\n",
            "Epoch 59/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.9397 - accuracy: 0.3834\n",
            "Epoch 60/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.9129 - accuracy: 0.3884\n",
            "Epoch 61/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.8949 - accuracy: 0.3910\n",
            "Epoch 62/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.8644 - accuracy: 0.3981\n",
            "Epoch 63/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.8395 - accuracy: 0.4037\n",
            "Epoch 64/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.7995 - accuracy: 0.4144\n",
            "Epoch 65/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.7832 - accuracy: 0.4182\n",
            "Epoch 66/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.7460 - accuracy: 0.4248\n",
            "Epoch 67/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.7176 - accuracy: 0.4336\n",
            "Epoch 68/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.6890 - accuracy: 0.4365\n",
            "Epoch 69/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.6694 - accuracy: 0.4457\n",
            "Epoch 70/660\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 2.6411 - accuracy: 0.4515\n",
            "Epoch 71/660\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 2.6231 - accuracy: 0.4542\n",
            "Epoch 72/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.6009 - accuracy: 0.4564\n",
            "Epoch 73/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.5756 - accuracy: 0.4599\n",
            "Epoch 74/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.5380 - accuracy: 0.4688\n",
            "Epoch 75/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.5282 - accuracy: 0.4752\n",
            "Epoch 76/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.5047 - accuracy: 0.4790\n",
            "Epoch 77/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.4966 - accuracy: 0.4791\n",
            "Epoch 78/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.4508 - accuracy: 0.4939\n",
            "Epoch 79/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.4471 - accuracy: 0.4900\n",
            "Epoch 80/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.4167 - accuracy: 0.4986\n",
            "Epoch 81/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.4012 - accuracy: 0.5043\n",
            "Epoch 82/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.3881 - accuracy: 0.5068\n",
            "Epoch 83/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.3562 - accuracy: 0.5101\n",
            "Epoch 84/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.3472 - accuracy: 0.5138\n",
            "Epoch 85/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.3336 - accuracy: 0.5178\n",
            "Epoch 86/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 2.3018 - accuracy: 0.5255\n",
            "Epoch 87/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.2787 - accuracy: 0.5330\n",
            "Epoch 88/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.2713 - accuracy: 0.5284\n",
            "Epoch 89/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.2571 - accuracy: 0.5421\n",
            "Epoch 90/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.2279 - accuracy: 0.5449\n",
            "Epoch 91/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.2151 - accuracy: 0.5454\n",
            "Epoch 92/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.2122 - accuracy: 0.5437\n",
            "Epoch 93/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.1931 - accuracy: 0.5494\n",
            "Epoch 94/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.1771 - accuracy: 0.5549\n",
            "Epoch 95/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 2.1575 - accuracy: 0.5554\n",
            "Epoch 96/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.1397 - accuracy: 0.5623\n",
            "Epoch 97/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.1273 - accuracy: 0.5657\n",
            "Epoch 98/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.1113 - accuracy: 0.5705\n",
            "Epoch 99/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0912 - accuracy: 0.5712\n",
            "Epoch 100/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0928 - accuracy: 0.5690\n",
            "Epoch 101/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0788 - accuracy: 0.5766\n",
            "Epoch 102/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0525 - accuracy: 0.5826\n",
            "Epoch 103/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0468 - accuracy: 0.5829\n",
            "Epoch 104/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0331 - accuracy: 0.5877\n",
            "Epoch 105/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0296 - accuracy: 0.5843\n",
            "Epoch 106/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 2.0065 - accuracy: 0.5936\n",
            "Epoch 107/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9766 - accuracy: 0.5988\n",
            "Epoch 108/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9711 - accuracy: 0.6061\n",
            "Epoch 109/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.9742 - accuracy: 0.6019\n",
            "Epoch 110/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9566 - accuracy: 0.6044\n",
            "Epoch 111/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9445 - accuracy: 0.6086\n",
            "Epoch 112/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9325 - accuracy: 0.6109\n",
            "Epoch 113/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9317 - accuracy: 0.6070\n",
            "Epoch 114/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9145 - accuracy: 0.6134\n",
            "Epoch 115/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.9008 - accuracy: 0.6158\n",
            "Epoch 116/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8863 - accuracy: 0.6211\n",
            "Epoch 117/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8846 - accuracy: 0.6197\n",
            "Epoch 118/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8855 - accuracy: 0.6180\n",
            "Epoch 119/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8496 - accuracy: 0.6251\n",
            "Epoch 120/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8498 - accuracy: 0.6275\n",
            "Epoch 121/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8446 - accuracy: 0.6295\n",
            "Epoch 122/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.8166 - accuracy: 0.6387\n",
            "Epoch 123/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.8030 - accuracy: 0.6379\n",
            "Epoch 124/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.7969 - accuracy: 0.6397\n",
            "Epoch 125/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8084 - accuracy: 0.6359\n",
            "Epoch 126/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.8094 - accuracy: 0.6334\n",
            "Epoch 127/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7715 - accuracy: 0.6441\n",
            "Epoch 128/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7739 - accuracy: 0.6419\n",
            "Epoch 129/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7539 - accuracy: 0.6521\n",
            "Epoch 130/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7581 - accuracy: 0.6449\n",
            "Epoch 131/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7357 - accuracy: 0.6553\n",
            "Epoch 132/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7500 - accuracy: 0.6506\n",
            "Epoch 133/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7230 - accuracy: 0.6537\n",
            "Epoch 134/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7090 - accuracy: 0.6583\n",
            "Epoch 135/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7090 - accuracy: 0.6568\n",
            "Epoch 136/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6978 - accuracy: 0.6614\n",
            "Epoch 137/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6987 - accuracy: 0.6581\n",
            "Epoch 138/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.7034 - accuracy: 0.6591\n",
            "Epoch 139/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6875 - accuracy: 0.6649\n",
            "Epoch 140/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6637 - accuracy: 0.6665\n",
            "Epoch 141/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6630 - accuracy: 0.6672\n",
            "Epoch 142/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6683 - accuracy: 0.6704\n",
            "Epoch 143/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6510 - accuracy: 0.6719\n",
            "Epoch 144/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6438 - accuracy: 0.6715\n",
            "Epoch 145/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6312 - accuracy: 0.6762\n",
            "Epoch 146/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6385 - accuracy: 0.6714\n",
            "Epoch 147/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6289 - accuracy: 0.6774\n",
            "Epoch 148/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6109 - accuracy: 0.6817\n",
            "Epoch 149/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6151 - accuracy: 0.6777\n",
            "Epoch 150/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.6002 - accuracy: 0.6817\n",
            "Epoch 151/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6023 - accuracy: 0.6794\n",
            "Epoch 152/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.6025 - accuracy: 0.6830\n",
            "Epoch 153/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5878 - accuracy: 0.6842\n",
            "Epoch 154/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5718 - accuracy: 0.6910\n",
            "Epoch 155/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5771 - accuracy: 0.6885\n",
            "Epoch 156/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5620 - accuracy: 0.6881\n",
            "Epoch 157/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5670 - accuracy: 0.6879\n",
            "Epoch 158/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5543 - accuracy: 0.6891\n",
            "Epoch 159/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5504 - accuracy: 0.6923\n",
            "Epoch 160/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5443 - accuracy: 0.6936\n",
            "Epoch 161/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5257 - accuracy: 0.6992\n",
            "Epoch 162/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5386 - accuracy: 0.6938\n",
            "Epoch 163/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.5252 - accuracy: 0.6991\n",
            "Epoch 164/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5075 - accuracy: 0.7027\n",
            "Epoch 165/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5184 - accuracy: 0.6978\n",
            "Epoch 166/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5124 - accuracy: 0.7042\n",
            "Epoch 167/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5059 - accuracy: 0.6994\n",
            "Epoch 168/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4886 - accuracy: 0.7053\n",
            "Epoch 169/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.5028 - accuracy: 0.7039\n",
            "Epoch 170/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4915 - accuracy: 0.7060\n",
            "Epoch 171/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4886 - accuracy: 0.7042\n",
            "Epoch 172/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4892 - accuracy: 0.7057\n",
            "Epoch 173/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4739 - accuracy: 0.7066\n",
            "Epoch 174/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4760 - accuracy: 0.7066\n",
            "Epoch 175/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4527 - accuracy: 0.7118\n",
            "Epoch 176/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4663 - accuracy: 0.7098\n",
            "Epoch 177/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.4725 - accuracy: 0.7070\n",
            "Epoch 178/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4430 - accuracy: 0.7166\n",
            "Epoch 179/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4470 - accuracy: 0.7111\n",
            "Epoch 180/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4407 - accuracy: 0.7142\n",
            "Epoch 181/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4322 - accuracy: 0.7183\n",
            "Epoch 182/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4491 - accuracy: 0.7133\n",
            "Epoch 183/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4319 - accuracy: 0.7155\n",
            "Epoch 184/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4222 - accuracy: 0.7186\n",
            "Epoch 185/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4162 - accuracy: 0.7214\n",
            "Epoch 186/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4175 - accuracy: 0.7210\n",
            "Epoch 187/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4117 - accuracy: 0.7229\n",
            "Epoch 188/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4200 - accuracy: 0.7205\n",
            "Epoch 189/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4021 - accuracy: 0.7222\n",
            "Epoch 190/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.4017 - accuracy: 0.7235\n",
            "Epoch 191/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4031 - accuracy: 0.7247\n",
            "Epoch 192/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.4030 - accuracy: 0.7249\n",
            "Epoch 193/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3960 - accuracy: 0.7252\n",
            "Epoch 194/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3754 - accuracy: 0.7287\n",
            "Epoch 195/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3694 - accuracy: 0.7334\n",
            "Epoch 196/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3747 - accuracy: 0.7294\n",
            "Epoch 197/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3739 - accuracy: 0.7268\n",
            "Epoch 198/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3806 - accuracy: 0.7262\n",
            "Epoch 199/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3707 - accuracy: 0.7279\n",
            "Epoch 200/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3745 - accuracy: 0.7289\n",
            "Epoch 201/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3575 - accuracy: 0.7325\n",
            "Epoch 202/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3612 - accuracy: 0.7293\n",
            "Epoch 203/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3627 - accuracy: 0.7282\n",
            "Epoch 204/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3488 - accuracy: 0.7304\n",
            "Epoch 205/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3472 - accuracy: 0.7335\n",
            "Epoch 206/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3383 - accuracy: 0.7335\n",
            "Epoch 207/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3451 - accuracy: 0.7340\n",
            "Epoch 208/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3296 - accuracy: 0.7364\n",
            "Epoch 209/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3396 - accuracy: 0.7344\n",
            "Epoch 210/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3326 - accuracy: 0.7328\n",
            "Epoch 211/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3308 - accuracy: 0.7395\n",
            "Epoch 212/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3313 - accuracy: 0.7359\n",
            "Epoch 213/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3122 - accuracy: 0.7436\n",
            "Epoch 214/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3139 - accuracy: 0.7433\n",
            "Epoch 215/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3278 - accuracy: 0.7377\n",
            "Epoch 216/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3131 - accuracy: 0.7412\n",
            "Epoch 217/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3092 - accuracy: 0.7416\n",
            "Epoch 218/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3074 - accuracy: 0.7423\n",
            "Epoch 219/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2959 - accuracy: 0.7460\n",
            "Epoch 220/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2950 - accuracy: 0.7430\n",
            "Epoch 221/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.3105 - accuracy: 0.7397\n",
            "Epoch 222/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2958 - accuracy: 0.7421\n",
            "Epoch 223/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3000 - accuracy: 0.7412\n",
            "Epoch 224/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2924 - accuracy: 0.7406\n",
            "Epoch 225/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2885 - accuracy: 0.7456\n",
            "Epoch 226/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2902 - accuracy: 0.7469\n",
            "Epoch 227/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2941 - accuracy: 0.7436\n",
            "Epoch 228/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2819 - accuracy: 0.7478\n",
            "Epoch 229/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2574 - accuracy: 0.7508\n",
            "Epoch 230/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2822 - accuracy: 0.7440\n",
            "Epoch 231/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2756 - accuracy: 0.7463\n",
            "Epoch 232/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2694 - accuracy: 0.7476\n",
            "Epoch 233/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2680 - accuracy: 0.7476\n",
            "Epoch 234/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.3001 - accuracy: 0.7387\n",
            "Epoch 235/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2774 - accuracy: 0.7461\n",
            "Epoch 236/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2551 - accuracy: 0.7522\n",
            "Epoch 237/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2617 - accuracy: 0.7485\n",
            "Epoch 238/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2524 - accuracy: 0.7494\n",
            "Epoch 239/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2730 - accuracy: 0.7487\n",
            "Epoch 240/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2401 - accuracy: 0.7539\n",
            "Epoch 241/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2464 - accuracy: 0.7514\n",
            "Epoch 242/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2464 - accuracy: 0.7531\n",
            "Epoch 243/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2545 - accuracy: 0.7492\n",
            "Epoch 244/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2615 - accuracy: 0.7461\n",
            "Epoch 245/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2516 - accuracy: 0.7506\n",
            "Epoch 246/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2307 - accuracy: 0.7567\n",
            "Epoch 247/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2231 - accuracy: 0.7586\n",
            "Epoch 248/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2334 - accuracy: 0.7570\n",
            "Epoch 249/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2179 - accuracy: 0.7603\n",
            "Epoch 250/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2336 - accuracy: 0.7546\n",
            "Epoch 251/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2505 - accuracy: 0.7490\n",
            "Epoch 252/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2186 - accuracy: 0.7604\n",
            "Epoch 253/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2154 - accuracy: 0.7595\n",
            "Epoch 254/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2200 - accuracy: 0.7582\n",
            "Epoch 255/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2239 - accuracy: 0.7544\n",
            "Epoch 256/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2444 - accuracy: 0.7518\n",
            "Epoch 257/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2080 - accuracy: 0.7597\n",
            "Epoch 258/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.2128 - accuracy: 0.7614\n",
            "Epoch 259/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2008 - accuracy: 0.7632\n",
            "Epoch 260/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2264 - accuracy: 0.7547\n",
            "Epoch 261/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2192 - accuracy: 0.7557\n",
            "Epoch 262/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2119 - accuracy: 0.7584\n",
            "Epoch 263/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2043 - accuracy: 0.7600\n",
            "Epoch 264/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1967 - accuracy: 0.7616\n",
            "Epoch 265/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1970 - accuracy: 0.7629\n",
            "Epoch 266/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2015 - accuracy: 0.7604\n",
            "Epoch 267/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.2063 - accuracy: 0.7581\n",
            "Epoch 268/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2088 - accuracy: 0.7579\n",
            "Epoch 269/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1926 - accuracy: 0.7632\n",
            "Epoch 270/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1817 - accuracy: 0.7626\n",
            "Epoch 271/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1729 - accuracy: 0.7692\n",
            "Epoch 272/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1864 - accuracy: 0.7645\n",
            "Epoch 273/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2115 - accuracy: 0.7566\n",
            "Epoch 274/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.2053 - accuracy: 0.7584\n",
            "Epoch 275/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1918 - accuracy: 0.7623\n",
            "Epoch 276/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1537 - accuracy: 0.7724\n",
            "Epoch 277/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1666 - accuracy: 0.7703\n",
            "Epoch 278/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1819 - accuracy: 0.7621\n",
            "Epoch 279/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1893 - accuracy: 0.7612\n",
            "Epoch 280/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1841 - accuracy: 0.7628\n",
            "Epoch 281/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1718 - accuracy: 0.7652\n",
            "Epoch 282/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1774 - accuracy: 0.7626\n",
            "Epoch 283/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1846 - accuracy: 0.7634\n",
            "Epoch 284/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1649 - accuracy: 0.7670\n",
            "Epoch 285/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1617 - accuracy: 0.7683\n",
            "Epoch 286/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1527 - accuracy: 0.7681\n",
            "Epoch 287/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1559 - accuracy: 0.7681\n",
            "Epoch 288/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1613 - accuracy: 0.7674\n",
            "Epoch 289/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1608 - accuracy: 0.7679\n",
            "Epoch 290/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1509 - accuracy: 0.7709\n",
            "Epoch 291/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1603 - accuracy: 0.7669\n",
            "Epoch 292/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1588 - accuracy: 0.7700\n",
            "Epoch 293/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1635 - accuracy: 0.7657\n",
            "Epoch 294/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1511 - accuracy: 0.7747\n",
            "Epoch 295/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1492 - accuracy: 0.7687\n",
            "Epoch 296/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1705 - accuracy: 0.7602\n",
            "Epoch 297/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1426 - accuracy: 0.7723\n",
            "Epoch 298/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1613 - accuracy: 0.7682\n",
            "Epoch 299/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1475 - accuracy: 0.7708\n",
            "Epoch 300/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1506 - accuracy: 0.7710\n",
            "Epoch 301/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1514 - accuracy: 0.7703\n",
            "Epoch 302/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1416 - accuracy: 0.7693\n",
            "Epoch 303/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1425 - accuracy: 0.7714\n",
            "Epoch 304/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1527 - accuracy: 0.7692\n",
            "Epoch 305/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1234 - accuracy: 0.7756\n",
            "Epoch 306/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1358 - accuracy: 0.7716\n",
            "Epoch 307/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1432 - accuracy: 0.7679\n",
            "Epoch 308/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1197 - accuracy: 0.7755\n",
            "Epoch 309/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1677 - accuracy: 0.7641\n",
            "Epoch 310/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1278 - accuracy: 0.7747\n",
            "Epoch 311/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1339 - accuracy: 0.7701\n",
            "Epoch 312/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1451 - accuracy: 0.7701\n",
            "Epoch 313/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1440 - accuracy: 0.7712\n",
            "Epoch 314/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1338 - accuracy: 0.7693\n",
            "Epoch 315/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1228 - accuracy: 0.7720\n",
            "Epoch 316/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1217 - accuracy: 0.7762\n",
            "Epoch 317/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1171 - accuracy: 0.7757\n",
            "Epoch 318/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1630 - accuracy: 0.7634\n",
            "Epoch 319/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1215 - accuracy: 0.7758\n",
            "Epoch 320/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1283 - accuracy: 0.7716\n",
            "Epoch 321/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1023 - accuracy: 0.7786\n",
            "Epoch 322/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.1217 - accuracy: 0.7776\n",
            "Epoch 323/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1144 - accuracy: 0.7780\n",
            "Epoch 324/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1233 - accuracy: 0.7707\n",
            "Epoch 325/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1225 - accuracy: 0.7738\n",
            "Epoch 326/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1243 - accuracy: 0.7744\n",
            "Epoch 327/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1102 - accuracy: 0.7768\n",
            "Epoch 328/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1000 - accuracy: 0.7804\n",
            "Epoch 329/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1090 - accuracy: 0.7772\n",
            "Epoch 330/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1028 - accuracy: 0.7806\n",
            "Epoch 331/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1121 - accuracy: 0.7767\n",
            "Epoch 332/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1039 - accuracy: 0.7778\n",
            "Epoch 333/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1079 - accuracy: 0.7765\n",
            "Epoch 334/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1052 - accuracy: 0.7771\n",
            "Epoch 335/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1253 - accuracy: 0.7725\n",
            "Epoch 336/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1011 - accuracy: 0.7771\n",
            "Epoch 337/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1004 - accuracy: 0.7793\n",
            "Epoch 338/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0911 - accuracy: 0.7788\n",
            "Epoch 339/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1040 - accuracy: 0.7770\n",
            "Epoch 340/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0950 - accuracy: 0.7773\n",
            "Epoch 341/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1089 - accuracy: 0.7756\n",
            "Epoch 342/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0919 - accuracy: 0.7794\n",
            "Epoch 343/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1121 - accuracy: 0.7738\n",
            "Epoch 344/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0912 - accuracy: 0.7815\n",
            "Epoch 345/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1065 - accuracy: 0.7767\n",
            "Epoch 346/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.1070 - accuracy: 0.7760\n",
            "Epoch 347/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0937 - accuracy: 0.7806\n",
            "Epoch 348/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0785 - accuracy: 0.7848\n",
            "Epoch 349/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0878 - accuracy: 0.7784\n",
            "Epoch 350/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.1041 - accuracy: 0.7757\n",
            "Epoch 351/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0905 - accuracy: 0.7789\n",
            "Epoch 352/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0788 - accuracy: 0.7833\n",
            "Epoch 353/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0917 - accuracy: 0.7813\n",
            "Epoch 354/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0878 - accuracy: 0.7822\n",
            "Epoch 355/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0869 - accuracy: 0.7769\n",
            "Epoch 356/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0969 - accuracy: 0.7768\n",
            "Epoch 357/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0960 - accuracy: 0.7770\n",
            "Epoch 358/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0768 - accuracy: 0.7831\n",
            "Epoch 359/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0781 - accuracy: 0.7841\n",
            "Epoch 360/660\n",
            "484/484 [==============================] - 4s 9ms/step - loss: 1.0890 - accuracy: 0.7791\n",
            "Epoch 361/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0884 - accuracy: 0.7795\n",
            "Epoch 362/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0893 - accuracy: 0.7783\n",
            "Epoch 363/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0889 - accuracy: 0.7794\n",
            "Epoch 364/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0969 - accuracy: 0.7759\n",
            "Epoch 365/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0748 - accuracy: 0.7827\n",
            "Epoch 366/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0835 - accuracy: 0.7764\n",
            "Epoch 367/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0657 - accuracy: 0.7838\n",
            "Epoch 368/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0604 - accuracy: 0.7868\n",
            "Epoch 369/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0635 - accuracy: 0.7866\n",
            "Epoch 370/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0817 - accuracy: 0.7815\n",
            "Epoch 371/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0705 - accuracy: 0.7870\n",
            "Epoch 372/660\n",
            "484/484 [==============================] - 5s 9ms/step - loss: 1.0646 - accuracy: 0.7844\n",
            "Epoch 373/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0801 - accuracy: 0.7804\n",
            "Epoch 374/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0843 - accuracy: 0.7771\n",
            "Epoch 375/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0922 - accuracy: 0.7784\n",
            "Epoch 376/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0754 - accuracy: 0.7806\n",
            "Epoch 377/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0591 - accuracy: 0.7855\n",
            "Epoch 378/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0575 - accuracy: 0.7843\n",
            "Epoch 379/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0598 - accuracy: 0.7869\n",
            "Epoch 380/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0651 - accuracy: 0.7873\n",
            "Epoch 381/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0663 - accuracy: 0.7819\n",
            "Epoch 382/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0640 - accuracy: 0.7872\n",
            "Epoch 383/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0538 - accuracy: 0.7871\n",
            "Epoch 384/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0377 - accuracy: 0.7934\n",
            "Epoch 385/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0586 - accuracy: 0.7840\n",
            "Epoch 386/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0666 - accuracy: 0.7861\n",
            "Epoch 387/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0786 - accuracy: 0.7835\n",
            "Epoch 388/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0563 - accuracy: 0.7847\n",
            "Epoch 389/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0542 - accuracy: 0.7837\n",
            "Epoch 390/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0450 - accuracy: 0.7887\n",
            "Epoch 391/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0623 - accuracy: 0.7827\n",
            "Epoch 392/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0731 - accuracy: 0.7818\n",
            "Epoch 393/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0510 - accuracy: 0.7852\n",
            "Epoch 394/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0618 - accuracy: 0.7826\n",
            "Epoch 395/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0599 - accuracy: 0.7839\n",
            "Epoch 396/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0457 - accuracy: 0.7889\n",
            "Epoch 397/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0400 - accuracy: 0.7906\n",
            "Epoch 398/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0469 - accuracy: 0.7848\n",
            "Epoch 399/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0819 - accuracy: 0.7760\n",
            "Epoch 400/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0455 - accuracy: 0.7883\n",
            "Epoch 401/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0323 - accuracy: 0.7917\n",
            "Epoch 402/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0386 - accuracy: 0.7902\n",
            "Epoch 403/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0340 - accuracy: 0.7884\n",
            "Epoch 404/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0550 - accuracy: 0.7859\n",
            "Epoch 405/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0594 - accuracy: 0.7833\n",
            "Epoch 406/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0474 - accuracy: 0.7853\n",
            "Epoch 407/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0434 - accuracy: 0.7866\n",
            "Epoch 408/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0799 - accuracy: 0.7811\n",
            "Epoch 409/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0550 - accuracy: 0.7863\n",
            "Epoch 410/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0255 - accuracy: 0.7921\n",
            "Epoch 411/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0224 - accuracy: 0.7923\n",
            "Epoch 412/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0499 - accuracy: 0.7852\n",
            "Epoch 413/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0304 - accuracy: 0.7905\n",
            "Epoch 414/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0570 - accuracy: 0.7824\n",
            "Epoch 415/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0573 - accuracy: 0.7841\n",
            "Epoch 416/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0336 - accuracy: 0.7902\n",
            "Epoch 417/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0436 - accuracy: 0.7868\n",
            "Epoch 418/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0335 - accuracy: 0.7886\n",
            "Epoch 419/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0235 - accuracy: 0.7943\n",
            "Epoch 420/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0317 - accuracy: 0.7905\n",
            "Epoch 421/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0526 - accuracy: 0.7861\n",
            "Epoch 422/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0354 - accuracy: 0.7898\n",
            "Epoch 423/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0430 - accuracy: 0.7847\n",
            "Epoch 424/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0272 - accuracy: 0.7924\n",
            "Epoch 425/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0474 - accuracy: 0.7837\n",
            "Epoch 426/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0209 - accuracy: 0.7938\n",
            "Epoch 427/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0287 - accuracy: 0.7914\n",
            "Epoch 428/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0418 - accuracy: 0.7875\n",
            "Epoch 429/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0354 - accuracy: 0.7896\n",
            "Epoch 430/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0416 - accuracy: 0.7855\n",
            "Epoch 431/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0305 - accuracy: 0.7887\n",
            "Epoch 432/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0162 - accuracy: 0.7916\n",
            "Epoch 433/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0307 - accuracy: 0.7898\n",
            "Epoch 434/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0253 - accuracy: 0.7905\n",
            "Epoch 435/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0329 - accuracy: 0.7852\n",
            "Epoch 436/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0394 - accuracy: 0.7853\n",
            "Epoch 437/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0365 - accuracy: 0.7861\n",
            "Epoch 438/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0271 - accuracy: 0.7884\n",
            "Epoch 439/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0187 - accuracy: 0.7930\n",
            "Epoch 440/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0074 - accuracy: 0.7939\n",
            "Epoch 441/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0159 - accuracy: 0.7930\n",
            "Epoch 442/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0253 - accuracy: 0.7917\n",
            "Epoch 443/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0405 - accuracy: 0.7897\n",
            "Epoch 444/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0218 - accuracy: 0.7924\n",
            "Epoch 445/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0292 - accuracy: 0.7884\n",
            "Epoch 446/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0267 - accuracy: 0.7907\n",
            "Epoch 447/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0306 - accuracy: 0.7868\n",
            "Epoch 448/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0360 - accuracy: 0.7824\n",
            "Epoch 449/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0152 - accuracy: 0.7924\n",
            "Epoch 450/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0121 - accuracy: 0.7924\n",
            "Epoch 451/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0184 - accuracy: 0.7895\n",
            "Epoch 452/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0319 - accuracy: 0.7886\n",
            "Epoch 453/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0131 - accuracy: 0.7939\n",
            "Epoch 454/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0217 - accuracy: 0.7894\n",
            "Epoch 455/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0090 - accuracy: 0.7943\n",
            "Epoch 456/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9992 - accuracy: 0.7962\n",
            "Epoch 457/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0148 - accuracy: 0.7914\n",
            "Epoch 458/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0255 - accuracy: 0.7883\n",
            "Epoch 459/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0357 - accuracy: 0.7849\n",
            "Epoch 460/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0110 - accuracy: 0.7928\n",
            "Epoch 461/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0027 - accuracy: 0.7963\n",
            "Epoch 462/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0188 - accuracy: 0.7906\n",
            "Epoch 463/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0211 - accuracy: 0.7927\n",
            "Epoch 464/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0138 - accuracy: 0.7922\n",
            "Epoch 465/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9960 - accuracy: 0.7970\n",
            "Epoch 466/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0274 - accuracy: 0.7886\n",
            "Epoch 467/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0097 - accuracy: 0.7950\n",
            "Epoch 468/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0197 - accuracy: 0.7885\n",
            "Epoch 469/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0117 - accuracy: 0.7934\n",
            "Epoch 470/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0148 - accuracy: 0.7888\n",
            "Epoch 471/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0091 - accuracy: 0.7948\n",
            "Epoch 472/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0022 - accuracy: 0.7953\n",
            "Epoch 473/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0071 - accuracy: 0.7939\n",
            "Epoch 474/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0014 - accuracy: 0.7953\n",
            "Epoch 475/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0029 - accuracy: 0.7958\n",
            "Epoch 476/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9974 - accuracy: 0.7925\n",
            "Epoch 477/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0022 - accuracy: 0.7934\n",
            "Epoch 478/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0019 - accuracy: 0.7938\n",
            "Epoch 479/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0224 - accuracy: 0.7913\n",
            "Epoch 480/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0135 - accuracy: 0.7913\n",
            "Epoch 481/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0153 - accuracy: 0.7908\n",
            "Epoch 482/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9905 - accuracy: 0.7989\n",
            "Epoch 483/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0063 - accuracy: 0.7938\n",
            "Epoch 484/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0014 - accuracy: 0.7954\n",
            "Epoch 485/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0033 - accuracy: 0.7935\n",
            "Epoch 486/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9955 - accuracy: 0.7932\n",
            "Epoch 487/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9944 - accuracy: 0.7951\n",
            "Epoch 488/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0115 - accuracy: 0.7917\n",
            "Epoch 489/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0085 - accuracy: 0.7925\n",
            "Epoch 490/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0053 - accuracy: 0.7919\n",
            "Epoch 491/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0182 - accuracy: 0.7906\n",
            "Epoch 492/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9871 - accuracy: 0.7959\n",
            "Epoch 493/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0085 - accuracy: 0.7925\n",
            "Epoch 494/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9886 - accuracy: 0.7970\n",
            "Epoch 495/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0025 - accuracy: 0.7914\n",
            "Epoch 496/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0167 - accuracy: 0.7893\n",
            "Epoch 497/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0070 - accuracy: 0.7934\n",
            "Epoch 498/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0082 - accuracy: 0.7918\n",
            "Epoch 499/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9922 - accuracy: 0.7964\n",
            "Epoch 500/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9911 - accuracy: 0.7923\n",
            "Epoch 501/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0002 - accuracy: 0.7943\n",
            "Epoch 502/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9946 - accuracy: 0.7959\n",
            "Epoch 503/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9877 - accuracy: 0.7962\n",
            "Epoch 504/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0014 - accuracy: 0.7937\n",
            "Epoch 505/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9887 - accuracy: 0.7966\n",
            "Epoch 506/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9918 - accuracy: 0.7966\n",
            "Epoch 507/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9948 - accuracy: 0.7924\n",
            "Epoch 508/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9986 - accuracy: 0.7924\n",
            "Epoch 509/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9869 - accuracy: 0.7976\n",
            "Epoch 510/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9879 - accuracy: 0.7954\n",
            "Epoch 511/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9950 - accuracy: 0.7936\n",
            "Epoch 512/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0093 - accuracy: 0.7906\n",
            "Epoch 513/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9879 - accuracy: 0.7957\n",
            "Epoch 514/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9766 - accuracy: 0.8003\n",
            "Epoch 515/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9854 - accuracy: 0.7971\n",
            "Epoch 516/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9898 - accuracy: 0.7949\n",
            "Epoch 517/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0004 - accuracy: 0.7928\n",
            "Epoch 518/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9968 - accuracy: 0.7930\n",
            "Epoch 519/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9954 - accuracy: 0.7933\n",
            "Epoch 520/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9941 - accuracy: 0.7941\n",
            "Epoch 521/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9879 - accuracy: 0.7972\n",
            "Epoch 522/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9689 - accuracy: 0.8025\n",
            "Epoch 523/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9818 - accuracy: 0.7954\n",
            "Epoch 524/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9746 - accuracy: 0.7970\n",
            "Epoch 525/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9835 - accuracy: 0.7960\n",
            "Epoch 526/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9949 - accuracy: 0.7925\n",
            "Epoch 527/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0084 - accuracy: 0.7887\n",
            "Epoch 528/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9984 - accuracy: 0.7971\n",
            "Epoch 529/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9971 - accuracy: 0.7931\n",
            "Epoch 530/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9732 - accuracy: 0.8007\n",
            "Epoch 531/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9789 - accuracy: 0.7979\n",
            "Epoch 532/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9727 - accuracy: 0.7987\n",
            "Epoch 533/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9822 - accuracy: 0.7969\n",
            "Epoch 534/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0013 - accuracy: 0.7912\n",
            "Epoch 535/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9823 - accuracy: 0.8003\n",
            "Epoch 536/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9742 - accuracy: 0.7991\n",
            "Epoch 537/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9738 - accuracy: 0.7980\n",
            "Epoch 538/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9755 - accuracy: 0.7989\n",
            "Epoch 539/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9756 - accuracy: 0.7982\n",
            "Epoch 540/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9904 - accuracy: 0.7941\n",
            "Epoch 541/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9936 - accuracy: 0.7946\n",
            "Epoch 542/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9825 - accuracy: 0.7957\n",
            "Epoch 543/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9851 - accuracy: 0.7958\n",
            "Epoch 544/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0121 - accuracy: 0.7879\n",
            "Epoch 545/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 1.0003 - accuracy: 0.7920\n",
            "Epoch 546/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9660 - accuracy: 0.8001\n",
            "Epoch 547/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9660 - accuracy: 0.8035\n",
            "Epoch 548/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9773 - accuracy: 0.7961\n",
            "Epoch 549/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9949 - accuracy: 0.7938\n",
            "Epoch 550/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9800 - accuracy: 0.7979\n",
            "Epoch 551/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9957 - accuracy: 0.7937\n",
            "Epoch 552/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9736 - accuracy: 0.7998\n",
            "Epoch 553/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9645 - accuracy: 0.7999\n",
            "Epoch 554/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9662 - accuracy: 0.7982\n",
            "Epoch 555/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9766 - accuracy: 0.7941\n",
            "Epoch 556/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9806 - accuracy: 0.7988\n",
            "Epoch 557/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9668 - accuracy: 0.7992\n",
            "Epoch 558/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9685 - accuracy: 0.8016\n",
            "Epoch 559/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9880 - accuracy: 0.7927\n",
            "Epoch 560/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9920 - accuracy: 0.7938\n",
            "Epoch 561/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9802 - accuracy: 0.7982\n",
            "Epoch 562/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9756 - accuracy: 0.7942\n",
            "Epoch 563/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9800 - accuracy: 0.7937\n",
            "Epoch 564/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9879 - accuracy: 0.7950\n",
            "Epoch 565/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9593 - accuracy: 0.8020\n",
            "Epoch 566/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9887 - accuracy: 0.7942\n",
            "Epoch 567/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9704 - accuracy: 0.7967\n",
            "Epoch 568/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9610 - accuracy: 0.8016\n",
            "Epoch 569/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9812 - accuracy: 0.7972\n",
            "Epoch 570/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9609 - accuracy: 0.7985\n",
            "Epoch 571/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9685 - accuracy: 0.8018\n",
            "Epoch 572/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9649 - accuracy: 0.7988\n",
            "Epoch 573/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9641 - accuracy: 0.7982\n",
            "Epoch 574/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9734 - accuracy: 0.7974\n",
            "Epoch 575/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9735 - accuracy: 0.7985\n",
            "Epoch 576/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9679 - accuracy: 0.8001\n",
            "Epoch 577/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9636 - accuracy: 0.8003\n",
            "Epoch 578/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9822 - accuracy: 0.7950\n",
            "Epoch 579/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9586 - accuracy: 0.8006\n",
            "Epoch 580/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9790 - accuracy: 0.7948\n",
            "Epoch 581/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9682 - accuracy: 0.7963\n",
            "Epoch 582/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9601 - accuracy: 0.8003\n",
            "Epoch 583/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9638 - accuracy: 0.8002\n",
            "Epoch 584/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9752 - accuracy: 0.7971\n",
            "Epoch 585/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9773 - accuracy: 0.7978\n",
            "Epoch 586/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9763 - accuracy: 0.7960\n",
            "Epoch 587/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9596 - accuracy: 0.7986\n",
            "Epoch 588/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9575 - accuracy: 0.8014\n",
            "Epoch 589/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9660 - accuracy: 0.7973\n",
            "Epoch 590/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9858 - accuracy: 0.7923\n",
            "Epoch 591/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9644 - accuracy: 0.7983\n",
            "Epoch 592/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9722 - accuracy: 0.7960\n",
            "Epoch 593/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9530 - accuracy: 0.8008\n",
            "Epoch 594/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9645 - accuracy: 0.8000\n",
            "Epoch 595/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9600 - accuracy: 0.7981\n",
            "Epoch 596/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9659 - accuracy: 0.8002\n",
            "Epoch 597/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9603 - accuracy: 0.7996\n",
            "Epoch 598/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9779 - accuracy: 0.7936\n",
            "Epoch 599/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9632 - accuracy: 0.8026\n",
            "Epoch 600/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9678 - accuracy: 0.7995\n",
            "Epoch 601/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9667 - accuracy: 0.7998\n",
            "Epoch 602/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9636 - accuracy: 0.8009\n",
            "Epoch 603/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9705 - accuracy: 0.7996\n",
            "Epoch 604/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9718 - accuracy: 0.7959\n",
            "Epoch 605/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9654 - accuracy: 0.7994\n",
            "Epoch 606/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9573 - accuracy: 0.8004\n",
            "Epoch 607/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9598 - accuracy: 0.8006\n",
            "Epoch 608/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9735 - accuracy: 0.7972\n",
            "Epoch 609/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9539 - accuracy: 0.8025\n",
            "Epoch 610/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9693 - accuracy: 0.7969\n",
            "Epoch 611/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9493 - accuracy: 0.8057\n",
            "Epoch 612/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9386 - accuracy: 0.8077\n",
            "Epoch 613/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9512 - accuracy: 0.8015\n",
            "Epoch 614/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9532 - accuracy: 0.8011\n",
            "Epoch 615/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9511 - accuracy: 0.8022\n",
            "Epoch 616/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9636 - accuracy: 0.7972\n",
            "Epoch 617/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9673 - accuracy: 0.7980\n",
            "Epoch 618/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9619 - accuracy: 0.7992\n",
            "Epoch 619/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9565 - accuracy: 0.8031\n",
            "Epoch 620/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9595 - accuracy: 0.7997\n",
            "Epoch 621/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9697 - accuracy: 0.8002\n",
            "Epoch 622/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9623 - accuracy: 0.7989\n",
            "Epoch 623/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9554 - accuracy: 0.8015\n",
            "Epoch 624/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9477 - accuracy: 0.8017\n",
            "Epoch 625/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9590 - accuracy: 0.7995\n",
            "Epoch 626/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9598 - accuracy: 0.7991\n",
            "Epoch 627/660\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 0.9540 - accuracy: 0.8033\n",
            "Epoch 628/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9527 - accuracy: 0.8025\n",
            "Epoch 629/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9459 - accuracy: 0.7993\n",
            "Epoch 630/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9493 - accuracy: 0.8024\n",
            "Epoch 631/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9520 - accuracy: 0.8024\n",
            "Epoch 632/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9558 - accuracy: 0.7997\n",
            "Epoch 633/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9506 - accuracy: 0.8025\n",
            "Epoch 634/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9635 - accuracy: 0.7997\n",
            "Epoch 635/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9594 - accuracy: 0.8002\n",
            "Epoch 636/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9467 - accuracy: 0.8025\n",
            "Epoch 637/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9662 - accuracy: 0.7973\n",
            "Epoch 638/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9679 - accuracy: 0.7960\n",
            "Epoch 639/660\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 0.9688 - accuracy: 0.7962\n",
            "Epoch 640/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9357 - accuracy: 0.8072\n",
            "Epoch 641/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9478 - accuracy: 0.8023\n",
            "Epoch 642/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9479 - accuracy: 0.8026\n",
            "Epoch 643/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9550 - accuracy: 0.8013\n",
            "Epoch 644/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9456 - accuracy: 0.8041\n",
            "Epoch 645/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9598 - accuracy: 0.7985\n",
            "Epoch 646/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9670 - accuracy: 0.7968\n",
            "Epoch 647/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9494 - accuracy: 0.8011\n",
            "Epoch 648/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9458 - accuracy: 0.8040\n",
            "Epoch 649/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9571 - accuracy: 0.8025\n",
            "Epoch 650/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9723 - accuracy: 0.7958\n",
            "Epoch 651/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9721 - accuracy: 0.7967\n",
            "Epoch 652/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9538 - accuracy: 0.8019\n",
            "Epoch 653/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9598 - accuracy: 0.7971\n",
            "Epoch 654/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9369 - accuracy: 0.8032\n",
            "Epoch 655/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9512 - accuracy: 0.8013\n",
            "Epoch 656/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9437 - accuracy: 0.8018\n",
            "Epoch 657/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9392 - accuracy: 0.8046\n",
            "Epoch 658/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9565 - accuracy: 0.7962\n",
            "Epoch 659/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9570 - accuracy: 0.7998\n",
            "Epoch 660/660\n",
            "484/484 [==============================] - 5s 10ms/step - loss: 0.9519 - accuracy: 0.8008\n"
          ]
        }
      ],
      "source": [
        "# Get the untrained model\n",
        "model = create_model(total_words, max_sequence_len)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(features, labels, epochs=660, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy72RPgly55q"
      },
      "source": [
        "**To pass this assignment, your model should achieve a training accuracy of at least 80%**. If your model didn't achieve this threshold, try training again with a different model architecture, consider increasing the number of unit in your `LSTM` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fXTEO3GJ282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "785630b6-1de7-4818-86f2-900a9356b54f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Take a look at the training curves of your model\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvED5A3qrn2"
      },
      "source": [
        "Before closing the assignment, be sure to also download the `history.pkl` file which contains the information of the training history of your model and will be used to compute your grade. You can download this file by running the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QRG73l6qE-c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d72afbfa-ba40-4bbd-d8b1-f4d472a032db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c8dd0654-3b49-4532-82ea-0fbb8222966f\", \"history.pkl\", 11924)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history.pkl')\n",
        "\n",
        "download_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdsMszk9zBs_"
      },
      "source": [
        "## See your model in action\n",
        "\n",
        "After all your work it is finally time to see your model generating text. \n",
        "\n",
        "Run the cell below to generate the next 100 words of a seed text.\n",
        "\n",
        "After submitting your assignment you are encouraged to try out training for different amounts of epochs and seeing how this affects the coherency of the generated text. Also try changing the seed text to see what you get!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vc6PHgxa6Hm"
      },
      "outputs": [],
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\t# Convert the text into sequences\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t# Pad the sequences\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\t# Get the probabilities of predicting a word\n",
        "\tpredicted = model.predict(token_list, verbose=0)\n",
        "\t# Choose the next word based on the maximum probability\n",
        "\tpredicted = np.argmax(predicted, axis=-1).item()\n",
        "\t# Get the actual word from the word index\n",
        "\toutput_word = tokenizer.index_word[predicted]\n",
        "\t# Append to the current text\n",
        "\tseed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQVDbdcYqSux"
      },
      "source": [
        "You will also need to submit this notebook for grading. To download it, click on the File tab in the upper left corner of the screen then click on Download -> Download .ipynb. You can name it anything you want as long as it is a valid .ipynb (jupyter notebook) file.\n",
        "\n",
        "**Congratulations on finishing this week's assignment!**\n",
        "\n",
        "You have successfully implemented a neural network capable of predicting the next word in a sequence of text!\n",
        "\n",
        "**We hope to see you in the next course of the specialization! Keep it up!**"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "main_language": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "name": "Predicting the next word.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}